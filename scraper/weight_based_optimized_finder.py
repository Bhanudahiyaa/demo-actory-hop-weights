#!/usr/bin/env python3
"""
Weight-Based Optimized Route Finder

This system combines:
1. Weight-based page prioritization (highest to lowest)
2. Optimized route finder's subpath memoization and compression
3. Navigation to every page with intelligent ordering
4. Routes displayed in format: (from this - to this - to this)
5. Sorted by hop count (ascending) and priority score (descending)
"""

import sys
import os
import time
from typing import List, Dict, Tuple, Optional, Generator, Set
from dataclasses import dataclass
from collections import defaultdict, deque
import psutil

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from scraper.neo4j_client import Neo4jClient


@dataclass
class PageInfo:
    """Information about a page including its weight and features."""
    url: str
    weight: int
    script_count: int
    form_count: int
    button_count: int
    input_count: int
    image_count: int
    link_count: int
    depth: int = 0
    visited: bool = False


@dataclass
class RouteInfo:
    """Information about a discovered route."""
    path: List[str]
    hop_count: int
    total_weight: int
    start_weight: int
    end_weight: int
    priority_score: float
    target_page: str


@dataclass
class RouteStats:
    """Statistics about route discovery performance."""
    total_routes: int
    compressed_routes: int
    max_depth: int
    discovery_time: float
    memory_usage_mb: float
    cache_hits: int
    cache_misses: int
    pages_covered: int
    total_pages: int
    coverage_percentage: float


class SubpathCache:
    """Efficient subpath memoization for route reuse."""
    
    def __init__(self):
        self.cache = defaultdict(dict)
        self.hits = 0
        self.misses = 0
    
    def get(self, start: str, end: str, max_depth: int) -> Optional[List[List[str]]]:
        """Get cached routes from start to end within max_depth."""
        if start in self.cache and end in self.cache[start]:
            routes = self.cache[start][end]
            # Filter by max_depth
            valid_routes = [route for route in routes if len(route) - 1 <= max_depth]
            if valid_routes:
                self.hits += 1
                return valid_routes
        self.misses += 1
        return None
    
    def set(self, start: str, end: str, routes: List[List[str]]):
        """Cache routes from start to end."""
        if start not in self.cache:
            self.cache[start] = {}
        self.cache[start][end] = routes
    
    def clear(self):
        """Clear the cache."""
        self.cache.clear()
        self.hits = 0
        self.misses = 0
    
    def get_stats(self) -> Tuple[int, int]:
        """Get cache hit/miss statistics."""
        return self.hits, self.misses


class WeightBasedOptimizedFinder:
    """Combined weight-based and optimized route finder."""
    
    def __init__(self, max_depth: int = 6, max_routes_per_target: int = 1000, 
                 enable_compression: bool = True, enable_memoization: bool = True):
        """Initialize the combined finder."""
        self.neo4j = Neo4jClient()
        self.subpath_cache = SubpathCache()
        self.max_depth = max_depth
        self.max_routes_per_target = max_routes_per_target
        self.enable_compression = enable_compression
        self.enable_memoization = enable_memoization
        self.pages_cache = {}
        self.routes_cache = {}
        
    def __enter__(self):
        return self
        
    def __exit__(self, exc_type, exc_val, exc_tb):
        self.close()
        
    def close(self):
        """Close Neo4j connection."""
        if hasattr(self, 'neo4j'):
            self.neo4j.close()
    
    def calculate_page_weight(self, page_data: dict) -> int:
        """Calculate page weight based on DOM features."""
        weight = 0
        
        # Scripts are highest priority (JavaScript-heavy pages)
        weight += page_data.get('scriptCount', 0) * 10
        
        # Forms indicate interactive functionality
        weight += page_data.get('formCount', 0) * 8
        
        # Buttons show user interaction points
        weight += page_data.get('buttonCount', 0) * 6
        
        # Inputs indicate data collection
        weight += page_data.get('inputCount', 0) * 5
        
        # Images show content richness
        weight += page_data.get('imageCount', 0) * 2
        
        # Links show navigation complexity
        weight += page_data.get('linkCount', 0) * 1
        
        return weight
    
    def fetch_pages_with_weights(self) -> List[PageInfo]:
        """Fetch all pages from Neo4j with calculated weights."""
        print("ğŸ“Š Fetching pages and calculating weights...")
        
        query = """
        MATCH (p:Page)
        RETURN p.url as url,
               COALESCE(p.scriptCount, 0) as scriptCount,
               COALESCE(p.formCount, 0) as formCount,
               COALESCE(p.buttonCount, 0) as buttonCount,
               COALESCE(p.inputCount, 0) as inputCount,
               COALESCE(p.imageCount, 0) as imageCount,
               COALESCE(p.linkCount, 0) as linkCount
        """
        
        pages = []
        with self.neo4j._driver.session() as session:
            result = session.run(query)
            for record in result:
                page_data = {
                    'scriptCount': record['scriptCount'],
                    'formCount': record['formCount'],
                    'buttonCount': record['buttonCount'],
                    'inputCount': record['inputCount'],
                    'imageCount': record['imageCount'],
                    'linkCount': record['linkCount']
                }
                
                weight = self.calculate_page_weight(page_data)
                page_info = PageInfo(
                    url=record['url'],
                    weight=weight,
                    script_count=page_data['scriptCount'],
                    form_count=page_data['formCount'],
                    button_count=page_data['buttonCount'],
                    input_count=page_data['inputCount'],
                    image_count=page_data['imageCount'],
                    link_count=page_data['linkCount']
                )
                pages.append(page_info)
        
        # Sort by weight (highest to lowest)
        pages.sort(key=lambda x: x.weight, reverse=True)
        
        print(f"âœ… Found {len(pages)} pages with weights")
        print(f"ğŸ† Top 10 pages by weight:")
        for i, page in enumerate(pages[:10]):
            print(f"   {i+1:2d}. {page.url} (Weight: {page.weight})")
        
        return pages
    
    def fetch_page_connections(self) -> Dict[str, List[str]]:
        """Fetch all page connections from Neo4j."""
        print("ğŸ”— Fetching page connections...")
        
        query = """
        MATCH (p1:Page)-[:LINKS_TO]->(p2:Page)
        RETURN p1.url as from_url, p2.url as to_url
        """
        
        connections = defaultdict(list)
        with self.neo4j._driver.session() as session:
            result = session.run(query)
            for record in result:
                from_url = record['from_url']
                to_url = record['to_url']
                connections[from_url].append(to_url)
        
        print(f"âœ… Found connections for {len(connections)} pages")
        return connections
    
    def find_routes_to_every_page(self, start_url: str, max_depth: int = None) -> List[RouteInfo]:
        """Find routes from start URL to every page, prioritized by weights."""
        if max_depth is None:
            max_depth = self.max_depth
            
        print(f"\nğŸ¯ Finding routes to EVERY PAGE from {start_url}")
        print(f"ğŸ“Š Max depth: {max_depth}")
        print(f"âš¡ Features: Memoization={'ON' if self.enable_memoization else 'OFF'}, "
              f"Compression={'ON' if self.enable_compression else 'OFF'}")
        print("-" * 80)
        
        start_time = time.time()
        
        # Fetch pages and connections
        pages = self.fetch_pages_with_weights()
        connections = self.fetch_page_connections()
        
        # Create URL to page info mapping
        url_to_page = {page.url: page for page in pages}
        
        # Find routes to each page, prioritized by weight
        all_routes = []
        pages_covered = 0
        
        print(f"\nğŸ” Discovering routes to {len(pages)} pages...")
        
        for i, target_page in enumerate(pages):
            if target_page.url == start_url:
                continue  # Skip start page
                
            print(f"   {i+1:2d}/{len(pages)-1}: Finding routes to {target_page.url} (Weight: {target_page.weight})")
            
            # Check cache first
            cached_routes = None
            if self.enable_memoization:
                cached_routes = self.subpath_cache.get(start_url, target_page.url, max_depth)
            
            if cached_routes:
                # Use cached routes
                for path in cached_routes:
                    route_info = self._create_route_info(path, url_to_page, target_page.url)
                    all_routes.append(route_info)
                print(f"      âœ… Found {len(cached_routes)} cached routes")
            else:
                # Find new routes
                routes_to_target = self._find_routes_to_single_target(
                    start_url, target_page.url, connections, url_to_page, max_depth
                )
                
                # Cache the routes
                if self.enable_memoization and routes_to_target:
                    paths = [route.path for route in routes_to_target]
                    self.subpath_cache.set(start_url, target_page.url, paths)
                
                all_routes.extend(routes_to_target)
                print(f"      âœ… Found {len(routes_to_target)} new routes")
            
            pages_covered += 1
            
            # Update progress
            if (i + 1) % 10 == 0 or i == len(pages) - 1:
                print(f"      ğŸ“Š Progress: {i+1}/{len(pages)-1} pages covered")
        
        # Sort routes by hop count (ascending) and then by priority score (descending)
        all_routes.sort(key=lambda x: (x.hop_count, -x.priority_score))
        
        discovery_time = time.time() - start_time
        
        print(f"\nâœ… Route discovery completed!")
        print(f"ğŸ“Š Total routes found: {len(all_routes):,}")
        print(f"ğŸ“„ Pages covered: {pages_covered}/{len(pages)} ({pages_covered/len(pages)*100:.1f}%)")
        print(f"â±ï¸  Discovery time: {discovery_time:.2f}s")
        
        return all_routes
    
    def _find_routes_to_single_target(self, start_url: str, target_url: str, 
                                    connections: Dict[str, List[str]], 
                                    url_to_page: Dict[str, PageInfo], 
                                    max_depth: int) -> List[RouteInfo]:
        """Find all routes from start to a single target using optimized BFS."""
        routes = []
        queue = deque([(start_url, [start_url], 0)])
        visited = set()
        
        while queue and len(routes) < self.max_routes_per_target:
            current_url, current_path, current_depth = queue.popleft()
            
            # Check if we reached the target
            if current_url == target_url and len(current_path) > 1:
                target_page = url_to_page.get(target_url)
                route_info = self._create_route_info(current_path, url_to_page, target_url)
                routes.append(route_info)
                continue
            
            # Stop if we've reached max depth
            if current_depth >= max_depth:
                continue
            
            # Explore neighbors
            for neighbor_url in connections.get(current_url, []):
                if neighbor_url not in visited:
                    visited.add(neighbor_url)
                    new_path = current_path + [neighbor_url]
                    queue.append((neighbor_url, new_path, current_depth + 1))
        
        return routes
    
    def _create_route_info(self, path: List[str], url_to_page: Dict[str, PageInfo], target_url: str) -> RouteInfo:
        """Create RouteInfo object from a path."""
        hop_count = len(path) - 1
        
        # Get page info for start and end
        start_page = url_to_page.get(path[0], PageInfo(path[0], 0, 0, 0, 0, 0, 0, 0))
        end_page = url_to_page.get(path[-1], PageInfo(path[-1], 0, 0, 0, 0, 0, 0, 0))
        
        # Calculate total weight and priority score
        total_weight = sum(url_to_page.get(url, PageInfo(url, 0, 0, 0, 0, 0, 0, 0)).weight for url in path)
        priority_score = (start_page.weight * 0.4) + (end_page.weight * 0.3) + (total_weight * 0.3)
        
        return RouteInfo(
            path=path,
            hop_count=hop_count,
            total_weight=total_weight,
            start_weight=start_page.weight,
            end_weight=end_page.weight,
            priority_score=priority_score,
            target_page=target_url
        )
    
    def display_routes_in_format(self, routes: List[RouteInfo], max_display: int = 100):
        """Display routes in the requested format: (from this - to this - to this)"""
        print(f"\nğŸ—ºï¸  DISPLAYING ROUTES (Format: from this - to this - to this)")
        print(f"ğŸ“Š Showing first {min(max_display, len(routes))} routes")
        print("=" * 100)
        
        # Group routes by hop count
        routes_by_hops = defaultdict(list)
        for route in routes:
            routes_by_hops[route.hop_count].append(route)
        
        # Display routes by hop count (ascending)
        for hop_count in sorted(routes_by_hops.keys()):
            hop_routes = routes_by_hops[hop_count]
            
            # Sort routes within same hop count by priority score
            hop_routes.sort(key=lambda x: -x.priority_score)
            
            print(f"\nğŸ”„ HOP COUNT: {hop_count} ({len(hop_routes)} routes)")
            print("-" * 80)
            
            for i, route in enumerate(hop_routes[:max_display]):
                # Format: (from this - to this - to this)
                formatted_path = " - ".join(route.path)
                print(f"   {i+1:3d}. ({formatted_path})")
                print(f"       ğŸ¯ Target: {route.target_page}")
                print(f"       ğŸ† Priority: {route.priority_score:.1f}, Weight: {route.total_weight}")
                
                if i >= max_display - 1:
                    remaining = len(hop_routes) - max_display
                    if remaining > 0:
                        print(f"       ... and {remaining} more routes")
                    break
    
    def get_route_summary(self, routes: List[RouteInfo]) -> RouteStats:
        """Get comprehensive statistics about the routes."""
        if not routes:
            return RouteStats(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
        
        # Calculate statistics
        total_routes = len(routes)
        max_depth = max(route.hop_count for route in routes) if routes else 0
        
        # Calculate memory usage
        process = psutil.Process()
        memory_usage_mb = process.memory_info().rss / 1024 / 1024
        
        # Get cache stats
        cache_hits, cache_misses = self.subpath_cache.get_stats()
        
        # Calculate coverage
        unique_targets = set(route.target_page for route in routes)
        pages_covered = len(unique_targets)
        
        # Get total pages
        pages = self.fetch_pages_with_weights()
        total_pages = len(pages)
        coverage_percentage = (pages_covered / total_pages * 100) if total_pages > 0 else 0
        
        return RouteStats(
            total_routes=total_routes,
            compressed_routes=total_routes,  # All routes are stored efficiently
            max_depth=max_depth,
            discovery_time=0,  # Will be set by caller
            memory_usage_mb=memory_usage_mb,
            cache_hits=cache_hits,
            cache_misses=cache_misses,
            pages_covered=pages_covered,
            total_pages=total_pages,
            coverage_percentage=coverage_percentage
        )
    
    def clear_cache(self):
        """Clear all caches."""
        self.subpath_cache.clear()
        self.pages_cache.clear()
        self.routes_cache.clear()
        print("ğŸ—‘ï¸  All caches cleared")


def main():
    """Example usage of the weight-based optimized route finder."""
    print("ğŸš€ WEIGHT-BASED OPTIMIZED ROUTE FINDER DEMO")
    print("=" * 70)
    print("This system combines weight-based prioritization with")
    print("optimized route discovery for comprehensive navigation.")
    print("=" * 70)
    
    # Initialize finder
    finder = WeightBasedOptimizedFinder(
        max_depth=6,
        max_routes_per_target=500,
        enable_compression=True,
        enable_memoization=True
    )
    
    try:
        # Find routes to every page
        print("\nğŸ¯ Finding routes to EVERY PAGE...")
        all_routes = finder.find_routes_to_every_page("https://zineps.com", max_depth=5)
        
        # Display routes in requested format
        finder.display_routes_in_format(all_routes, max_display=50)
        
        # Get comprehensive statistics
        print("\nğŸ“Š ROUTE DISCOVERY STATISTICS")
        print("-" * 50)
        
        stats = finder.get_route_summary(all_routes)
        print(f"ğŸ“„ Total Routes: {stats.total_routes:,}")
        print(f"ğŸ“„ Pages Covered: {stats.pages_covered}/{stats.total_pages} ({stats.coverage_percentage:.1f}%)")
        print(f"ğŸ“ Max Depth: {stats.max_depth}")
        print(f"ğŸ’¾ Memory Usage: {stats.memory_usage_mb:.2f} MB")
        print(f"ğŸ¯ Cache Hit Rate: {stats.cache_hits / (stats.cache_hits + stats.cache_misses) * 100:.1f}%")
        
        print(f"\nğŸ’¡ Key Benefits:")
        print(f"   â€¢ Weight-based prioritization (highest to lowest)")
        print(f"   â€¢ Routes to EVERY page discovered")
        print(f"   â€¢ Subpath memoization for efficiency")
        print(f"   â€¢ Route compression for memory optimization")
        print(f"   â€¢ Exact format: (from this - to this - to this)")
        print(f"   â€¢ Sorted by hop count and priority")
        
    finally:
        finder.close()


if __name__ == "__main__":
    main()
